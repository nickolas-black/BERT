# BERT
## Что такое BERT?
Двунаправленные представления кодировщика от трансформаторов (BERT) - это метод предварительного обучения NLP (обработка естественного языка), разработанный Google. BERT был создан и опубликован в 2018 году Якобом Девлином и его коллегами из Google. Google использует BERT, чтобы лучше понимать поисковые запросы пользователей.

BERT предназначен для предварительного обучения глубоких двунаправленных представлений из немаркированного текста путем совместной обработки левого и правого контекста на всех уровнях. В результате предварительно обученная модель BERT может быть настроена всего с одним дополнительным выходным слоем для создания современных моделей для широкого круга задач, таких как ответы на вопросы и логический вывод, без существенных модификаций архитектуры для конкретных задач.
